Homework 2
================
Amin Yakubu
9/30/2018

Problem 1
---------

Loading the tidyverse package

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.0.0     ✔ purrr   0.2.5
    ## ✔ tibble  1.4.2     ✔ dplyr   0.7.6
    ## ✔ tidyr   0.8.1     ✔ stringr 1.3.1
    ## ✔ readr   1.1.1     ✔ forcats 0.3.0

    ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

Reading the NYC transit dataset

``` r
transit_df = read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names(dat = .) %>% 
  select(line, station_name, ends_with("de"), route1:route11, entry, vending, entrance_type, ada ) %>% 
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   `Station Latitude` = col_double(),
    ##   `Station Longitude` = col_double(),
    ##   Route8 = col_integer(),
    ##   Route9 = col_integer(),
    ##   Route10 = col_integer(),
    ##   Route11 = col_integer(),
    ##   ADA = col_logical(),
    ##   `Free Crossover` = col_logical(),
    ##   `Entrance Latitude` = col_double(),
    ##   `Entrance Longitude` = col_double()
    ## )

    ## See spec(...) for full column specifications.

This dataset contains information about NYC Transit trains. The variables include details about stations, trains, routes served, vending information and ada compliance information.

The code chunk above imports the data and clean the variables names by replacing spaces with underscores and making the names lower case. We then select the variable we need. We've also changed the entry variable to logical type.

We have 1868 rows x 21 columns in our data. The data isn't tidy because same data spread across multiple columns for route. Also data (route number) is stored in the columns of route.

#### Unique stations

``` r
unique_df = distinct(transit_df, line, station_name, ada)

nrow(unique_df)
```

    ## [1] 465

``` r
sum(unique_df$ada)
```

    ## [1] 84

There are 465 stations. Of 465 stations, 84 are ada compliant.

We are using this code to find identify the stations with no vending (Vending=NO) but allows entrance (entry=YES)

``` r
vend_entry_df = transit_df %>% 
  mutate(vending = recode(vending, "YES" = TRUE, "NO" = FALSE),
         no_vend_yes_entry = !(vending == entry))
  
prop_vend_entry = sum(vend_entry_df$no_vend_yes_entry)/nrow(vend_entry_df)
prop_vend_entry
```

    ## [1] 0.03747323

We have 0.0374732 of entrance/exits with no vending allow entrance.

Reformatting the data in combination to finding number of stations that serve the A train

``` r
reformatted_df = transit_df %>% 
  gather(key = route , value = train , route1:route11) %>% 
  separate(route, into = c("remove1", "route"), sep = 5) %>% 
  select(everything(), -remove1) %>% 
  distinct(line, station_name, train, ada) %>% 
  filter(train == "A")
```

60 distinct stations serve the A train. Of the 60 trains, 17 are ada compliant.

Problem 2
---------

Reading and cleaning Mr. Trashwheel dataset.

``` r
trash_df = readxl::read_excel("./data/HealthyHarborWaterWheelTotals2017-9-26.xlsx", sheet = "Mr. Trash Wheel", cellranger::cell_cols(1:13)) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster), dumpster != 23220) %>% 
  mutate(sports_balls = as.integer(sports_balls))
```

Reading the 2016 & 2017 precipation data

``` r
prec_2016 = readxl::read_excel("./data/HealthyHarborWaterWheelTotals2017-9-26.xlsx", sheet = "2016 Precipitation", range = "A2:B14") %>% 
  janitor::clean_names() %>% 
  mutate(year = 2016)
  

prec_2017 = readxl::read_excel("./data/HealthyHarborWaterWheelTotals2017-9-26.xlsx", sheet = "2017 Precipitation", range = "A2:B14") %>% 
  janitor::clean_names() %>% 
  mutate(year = 2017) %>% 
  filter(!is.na(total))
```

Stacking the datasets prec\_2016 and prec\_2017

``` r
total_prec = bind_rows(prec_2016, prec_2017) 

## pulling the months as a vector from the tibble to be able to apply the month.name function
month_vector =  month.name[c(pull(total_prec, month))]

## coverting the vector into a tibble to be able to bind the columns
month_df = tibble::as.tibble(month_vector) 

## binding columns and renaming and selecting variables
precipitation_df = bind_cols(total_prec, month_df) %>% 
  select(everything(), -month) %>% 
  select(year, month = value, total)
```

Median for 2017 for sports balls

``` r
balls_median_17 = trash_df %>% 
  filter(year == 2017) 
```

The total precipitation in 2017 is 29.93 and the median number of sports balls is `median(balls_median_17$sports_balls)`

Problem 3
---------

``` r
devtools::install_github("p8105/p8105.datasets")
```

    ## Skipping install of 'p8105.datasets' from a github remote, the SHA1 (21f5ad1c) has not changed since last install.
    ##   Use `force = TRUE` to force installation

``` r
library(p8105.datasets)

data(brfss_smart2010)
```

#### Data tidying and wrangling

class, topic, question, sample size, and everything from lower confidence limit to GeoLocation s

``` r
brfss_df = janitor::clean_names(dat = brfss_smart2010) %>% 
  filter(topic == "Overall Health") %>% 
  select(-(class:question), -sample_size, -(confidence_limit_low:geo_location)) %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  mutate(excel_vgood = excellent + very_good)
```

Questions

There are 51 unique locations. Everything state is represented and Washington DC is also included.

``` r
most_observed = count(brfss_df, locationabbr) %>% 
  top_n(1, n) %>% 
  select(locationabbr)
```

NJ is most observed.

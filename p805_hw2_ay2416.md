Homework 2
================
Amin Yakubu
9/30/2018

Problem 1
---------

Loading the tidyverse package

``` r
library(tidyverse)
```

    ## ── Attaching packages ───────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.0.0     ✔ purrr   0.2.5
    ## ✔ tibble  1.4.2     ✔ dplyr   0.7.6
    ## ✔ tidyr   0.8.1     ✔ stringr 1.3.1
    ## ✔ readr   1.1.1     ✔ forcats 0.3.0

    ## ── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

Reading the NYC transit dataset

``` r
transit_df = read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names(dat = .) %>% 
  select(line, station_name, ends_with("de"), route1:route11, entry, vending, entrance_type, ada ) %>% 
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   `Station Latitude` = col_double(),
    ##   `Station Longitude` = col_double(),
    ##   Route8 = col_integer(),
    ##   Route9 = col_integer(),
    ##   Route10 = col_integer(),
    ##   Route11 = col_integer(),
    ##   ADA = col_logical(),
    ##   `Free Crossover` = col_logical(),
    ##   `Entrance Latitude` = col_double(),
    ##   `Entrance Longitude` = col_double()
    ## )

    ## See spec(...) for full column specifications.

This dataset contains information about NYC Transit trains. The variables include details about stations, trains, routes served, vending information and ada compliance information.

The code chunk above imports the data and clean the variables names by replacing spaces with underscores and making the names lower case. We then select the variable we need. We've also changed the entry variable to logical type.

We have 1868 rows x 21 columns in our data. The data isn't tidy because same data are spread across multiple columns for route. Also data (route number) is stored in the columns (in variable name) of route.

#### Unique stations and number that a ADA compliant

``` r
unique_df = distinct(transit_df, line, station_name, ada)

nrow(unique_df)
```

    ## [1] 465

``` r
sum(unique_df$ada)
```

    ## [1] 84

There are 465 stations. Of 465 stations, 84 are ada compliant.

Here, we use this code to identify the stations with no vending (Vending=NO) but allows entrance (entry=YES). By testing a logical variable. It's easier to just do the sum of logical variables to find the number of TRUEs so we negate the logical condition so that FALSES are converted to TRUE.

``` r
vend_entry_df = transit_df %>% 
  mutate(vending = recode(vending, "YES" = TRUE, "NO" = FALSE),
         no_vend_yes_entry = !(vending == entry)) %>% 
  filter(!(vending == TRUE & entry == FALSE))
  
sum(vend_entry_df$no_vend_yes_entry)/nrow(vend_entry_df)
```

    ## [1] 0.03695769

We have 0.0369577 of entrance/exits with no vending allow entrance.

Reformatting the data and combination to finding number of stations that serve the A train

``` r
reformatted_df = transit_df %>% 
  gather(key = route , value = train , route1:route11) %>% 
  separate(route, into = c("remove1", "route"), sep = 5) %>% 
  select(-remove1) %>% 
  distinct(line, station_name, train, ada) %>% 
  filter(train == "A")
```

60 distinct stations serve the A train. Of the 60 trains, 17 are ada compliant.

Problem 2
---------

Reading and cleaning Mr. Trashwheel dataset.

``` r
trash_df = readxl::read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N258") %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  mutate(sports_balls = as.integer(sports_balls))
```

Reading the 2016 & 2017 precipation data

``` r
prec_2016 = readxl::read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = "2016 Precipitation", range = "A2:B14") %>% 
  janitor::clean_names() %>% 
  mutate(year = 2016)
  

prec_2017 = readxl::read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = "2017 Precipitation", range = "A2:B14") %>% 
  janitor::clean_names() %>% 
  mutate(year = 2017) %>% 
  filter(!is.na(total))
```

Stacking the datasets prec\_2016 and prec\_2017

``` r
total_prec = bind_rows(prec_2016, prec_2017) 

## pulling the months as a vector from the tibble to be able to apply the month.name function
month_vector =  month.name[c(pull(total_prec, month))]

## coverting the vector into a tibble to be able to bind the columns
month_df = tibble::as.tibble(month_vector) 

## binding columns and renaming and selecting variables
precipitation_df = bind_cols(total_prec, month_df) %>% 
  select(-month) %>% 
  select(year, month = value, total)
```

Median for 2016 for sports balls

``` r
balls_median_16 = trash_df %>% 
  filter(year == 2016)
```

The trash wheel dataset contains 216 rows x 14 columns after cleaning. It contains a wide range of information about the trashwheel waste collector. There's information about the type of waste collected like number of plastic and glass bottles, sport balls, grocery bags etc. Date information is also collected.

The precipitation datasets contains information about precipation for the year 2016 and 2017. There are 12 rows (observations) x 3 columns in the 2016 precipation data. In the dataset for 2017, we have 12 rows (observations) x 3 columns. Both contain information on total precipation (total variable) of each month (month variable).

The total precipitation in 2017 is 32.93 and the median number of sports balls is 26

Problem 3
---------

``` r
devtools::install_github("p8105/p8105.datasets")
```

    ## Skipping install of 'p8105.datasets' from a github remote, the SHA1 (21f5ad1c) has not changed since last install.
    ##   Use `force = TRUE` to force installation

``` r
library(p8105.datasets)

data(brfss_smart2010)
```

#### Data tidying and wrangling

class, topic, question, sample size, and everything from lower confidence limit to GeoLocation s

``` r
brfss_df = janitor::clean_names(dat = brfss_smart2010) %>% 
  filter(topic == "Overall Health") %>% 
  select(-(class:question), -sample_size, -(confidence_limit_low:geo_location)) %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  mutate(excel_vgood = excellent + very_good)
```

Questions to be answered

There are 404 unique locations. There are 51 unique states. Every state is represented and Washington DC is also included.

``` r
most_observed = count(brfss_df, locationabbr) %>% 
  top_n(1, n) 
```

NJ is most observed with 146 observations/townships.

``` r
data_2002 = brfss_df %>% 
  filter(year == 2002)

median(data_2002$excellent, na.rm = TRUE)
```

    ## [1] 23.6

The median value of excellent response in the year 2002 is 23.6.

#### Plots

Histogram

``` r
ggplot(data_2002, aes(x = excellent)) + 
  geom_histogram()
```

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

    ## Warning: Removed 2 rows containing non-finite values (stat_bin).

![](p805_hw2_ay2416_files/figure-markdown_github/unnamed-chunk-12-1.png)

Scatterplot

``` r
brfss_df %>% 
  filter(locationdesc %in% c("NY - New York County", "NY - Queens County")) %>% 
  ggplot(aes(x = year, y = excellent, color = locationdesc)) +
  geom_point()
```

![](p805_hw2_ay2416_files/figure-markdown_github/unnamed-chunk-13-1.png)
